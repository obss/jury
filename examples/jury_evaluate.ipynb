{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/obss/jury/blob/main/examples/jury_evaluate.ipynb\"><img alt=\"Open in Colab\" src=\"https://colab.research.google.com/assets/colab-badge.svg\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Packages (Colab)\n",
    "\n",
    "To be able to use several metrics (e.g SacreBLEU, BERTScore, etc.), you need to install related package. When you try to use it without having those required packages, an exception will be thrown indicating that installation of spesific package is required. If you want to see score outputs for SacreBLEU and BERTScore in the experiments in this notebook, comment off related lines (those will be declared later with in line comments).\n",
    "\n",
    "If you want to see/use those metrics, install required packages below with commenting off the code cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install jury"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install sacrebleu bert-score==0.3.9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports\n",
    "\n",
    "We start with required imports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-02T19:59:22.515146Z",
     "start_time": "2021-10-02T19:59:20.716835Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json # Just for pretty printing the resulting dict.\n",
    "\n",
    "from jury import load_metric, Jury"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-02T19:59:22.531153Z",
     "start_time": "2021-10-02T19:59:22.516139Z"
    }
   },
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "\n",
    "def read_from_txt(path: str) -> List[str]:\n",
    "    with open(path, \"r\") as f:\n",
    "        data = f.readlines()\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: Machine Translation\n",
    "\n",
    "We evaluate sample machine translation generation outputs and their references. Feel free to play around with the samples below. Alternatively, you can load your own predictions and references using helper function `read_from_txt()`, where each line will be treated as a separate prediction or references, and order needs to be consistent between prediction and reference txt file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-02T19:59:22.547167Z",
     "start_time": "2021-10-02T19:59:22.532153Z"
    }
   },
   "outputs": [],
   "source": [
    "mt_predictions = [\n",
    "    [\"the cat is on the mat\", \"There is cat playing on the mat\"], \n",
    "    [\"Look! a wonderful day.\"]\n",
    "]\n",
    "mt_references = [\n",
    "    [\"the cat is playing on the mat.\", \"The cat plays on the mat.\"],\n",
    "    [\"Today is a wonderful day\", \"The weather outside is wonderful.\"],\n",
    "]\n",
    "\n",
    "# mt_predictions = read_from_txt(\"/path/to/predictions.txt\")\n",
    "# mt_references = read_from_txt(\"/path/to/references.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Metrics\n",
    "\n",
    "Here define your metrics used to evaluate MT prediction and references. You can either use load function from jury where you can pass additional parameters to specified metric, or specify as string, which will use default parameters.\n",
    "\n",
    "**NOTE:** Computation of BERTScore may take some time as it will download a model for computing embeddings. Thus, we here provide `albert-base-v1`, but you can uncomment the previous line where it uses default model `roberta-large`.\n",
    "\n",
    "[Here](https://huggingface.co/transformers/pretrained_models.html), you can observe model sizes, parameter counts, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-02T19:59:22.562180Z",
     "start_time": "2021-10-02T19:59:22.548168Z"
    }
   },
   "outputs": [],
   "source": [
    "MT_METRICS = [\n",
    "    load_metric(\"bleu\", resulting_name=\"bleu_1\", params={\"max_order\": 1}),\n",
    "    load_metric(\"bleu\", resulting_name=\"bleu_2\", params={\"max_order\": 2}),\n",
    "    load_metric(\"meteor\"),\n",
    "    load_metric(\"rouge\"),\n",
    "#     load_metric(\"sacrebleu\"),  # (optional)\n",
    "#     load_metric(\"bertscore\"), # (optional) Using default model for lang en\n",
    "#     load_metric(\"bertscore\", params={\"model_type\": \"albert-base-v1\"})  # (optional) Using smaller model to reduce download time.\n",
    "]\n",
    "\n",
    "# Alternatively\n",
    "# MT_METRICS = [\n",
    "#     \"bleu\",\n",
    "#     \"meteor\",\n",
    "#     \"rouge\"\n",
    "# ]\n",
    "\n",
    "RUN_CONCURRENT = True  # set False to disable concurrency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-02T19:59:26.664594Z",
     "start_time": "2021-10-02T19:59:22.563182Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Compute scores\n",
    "\n",
    "if RUN_CONCURRENT:\n",
    "    os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\"\n",
    "else:\n",
    "    os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "mt_jury = Jury(metrics=MT_METRICS, run_concurrent=RUN_CONCURRENT)\n",
    "scores = mt_jury.evaluate(predictions=mt_predictions, references=mt_references)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-02T19:59:26.679608Z",
     "start_time": "2021-10-02T19:59:26.665594Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Display results\n",
    "print(json.dumps(scores, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Question Answering\n",
    "\n",
    "For question answering task, commonly used evaluation metric is exact match or F1 score, datasets package allows this through a metric named \"squad\". Same interface is available here as well, with a single exception that in order to seamlessly compute, concat and output resulting scores Jury restrict each metric to compute a single score, by default SQUAD implementation computes (squad's) F1 score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-02T19:59:29.552689Z",
     "start_time": "2021-10-02T19:59:29.540678Z"
    }
   },
   "outputs": [],
   "source": [
    "qa_predictions = [\"1917\", \"Albert Einstein\", \"foo bar\"]\n",
    "qa_references = [\"1917\", \"Einstein\", \"foo bar foobar\"]\n",
    "\n",
    "QA_METRICS = [\n",
    "    \"squad\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-02T19:59:30.130953Z",
     "start_time": "2021-10-02T19:59:30.117949Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "qa_jury = Jury(metrics=QA_METRICS, run_concurrent=False)\n",
    "scores = qa_jury.evaluate(predictions=qa_predictions, references=qa_references)\n",
    "print(json.dumps(scores, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining a custom metric\n",
    "\n",
    "To define a custom metric, you only need to extend `jury.metrics.Metric` class and implement the required functions as desired. We create a metric to compute precision for our QA task above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-02T20:17:28.152162Z",
     "start_time": "2021-10-02T20:17:28.141152Z"
    }
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from typing import Callable\n",
    "\n",
    "import datasets\n",
    "from jury.collator import Collator\n",
    "from jury.metrics import Metric\n",
    "from jury.metrics._utils import normalize_text\n",
    "\n",
    "\n",
    "class WordMatch(Metric):\n",
    "    \"\"\"\n",
    "    Compute matching word ratio between prediction and reference \n",
    "        Average( # of matching tokens / # of unique tokens)\n",
    "    \n",
    "    Example:\n",
    "        pred = [\"foo bar foobar\"]\n",
    "        ref = [\"foo bar barfoo\"]\n",
    "        matching_tokens = 2 # 'foo' and 'bar'\n",
    "        total_tokens = 4 # 'foo', 'bar', 'foobar', and 'barfoo'\n",
    "        score = 0.5 # (2/4)\n",
    "    \"\"\"\n",
    "    def _info(self):\n",
    "        return datasets.MetricInfo(\n",
    "            description=\"Custom metric to compute ratio of matching tokens over unique tokens.\",\n",
    "            citation=\"Custom metric\",\n",
    "            inputs_description=\"\",\n",
    "            features=self.default_features,\n",
    "        )\n",
    "    \n",
    "    def _tokenize(self, predictions: Collator, references: Collator):\n",
    "        predictions = [normalize_text(p).split() for p in predictions]\n",
    "        references = [normalize_text(r).split() for r in references]\n",
    "        return predictions, references\n",
    "    \n",
    "    def _compute_single_pred_single_ref(\n",
    "        self, predictions: Collator, references: Collator, reduce_fn: Callable = None, **kwargs\n",
    "    ):\n",
    "        scores = []\n",
    "        predictions, references = self._tokenize(predictions, references)\n",
    "        for pred, ref in zip(predictions, references):\n",
    "            score = 0\n",
    "            pred_counts = Counter(pred)\n",
    "            ref_counts = Counter(ref)\n",
    "            total_unique_tokens = len(pred_counts + ref_counts)\n",
    "            for token, pred_count in pred_counts.items():\n",
    "                if token in ref_counts:\n",
    "                    score += min(pred_count, ref_counts[token])  # Intersection count\n",
    "            score = score / total_unique_tokens\n",
    "            scores.append(score)\n",
    "        avg_score = sum(scores) / len(scores)\n",
    "        return {\"score\": avg_score}\n",
    "\n",
    "    def _compute_single_pred_multi_ref(\n",
    "        self, predictions: Collator, references: Collator, reduce_fn: Callable, **kwargs\n",
    "    ):\n",
    "        scores = []\n",
    "        for pred, refs in zip(predictions, references):\n",
    "            pred_score = [\n",
    "                self._compute_single_pred_single_ref(Collator([pred], keep=True), Collator([ref], keep=True))\n",
    "                for ref in refs\n",
    "            ]\n",
    "            reduced_score = self._reduce_scores(pred_score, reduce_fn=reduce_fn)\n",
    "            scores.append(reduced_score)\n",
    "\n",
    "        return self._reduce_scores(scores, reduce_fn=np.mean)\n",
    "\n",
    "    def _compute_multi_pred_multi_ref(self, predictions: Collator, references: Collator, reduce_fn: Callable, **kwargs):\n",
    "        scores = []\n",
    "        for preds, refs in zip(predictions, references):\n",
    "            pred_scores = []\n",
    "            for pred in preds:\n",
    "                pred_score = self._compute_single_pred_multi_ref(\n",
    "                    Collator([pred], keep=True), Collator([refs], keep=True), reduce_fn=reduce_fn\n",
    "                )\n",
    "                pred_scores.append(pred_score)\n",
    "            reduced_score = self._reduce_scores(pred_scores, reduce_fn=reduce_fn)\n",
    "            scores.append(reduced_score)\n",
    "\n",
    "        return self._reduce_scores(scores, reduce_fn=np.mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-02T20:17:28.353117Z",
     "start_time": "2021-10-02T20:17:28.348113Z"
    }
   },
   "outputs": [],
   "source": [
    "from jury.metrics import Squad\n",
    "\n",
    "QA_METRICS = [\n",
    "    Squad(),\n",
    "    WordMatch()\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-02T20:17:28.583593Z",
     "start_time": "2021-10-02T20:17:28.556577Z"
    }
   },
   "outputs": [],
   "source": [
    "qa_jury = Jury(metrics=QA_METRICS, run_concurrent=False)\n",
    "scores = qa_jury.evaluate(predictions=qa_predictions, references=qa_references)\n",
    "print(json.dumps(scores, indent=4))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
