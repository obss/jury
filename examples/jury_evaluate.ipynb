{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports\n",
    "\n",
    "We start with required imports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-02T22:49:55.831703Z",
     "start_time": "2021-08-02T22:49:55.817682Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json # Just for pretty printing the resulting dict.\n",
    "\n",
    "from jury import Jury\n",
    "from jury.metrics import load_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "\n",
    "def read_from_txt(path: str) -> List[str]:\n",
    "    with open(path, \"r\") as f:\n",
    "        data = f.readlines()\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: Machine Translation\n",
    "\n",
    "We evaluate sample machine translation generation outputs and their references. Feel free to play around with the samples below. Alternatively, you can load your own predictions and references using helper function `read_from_txt()`, where each line will be treated as a separate prediction or references, and order needs to be consistent between prediction and reference txt file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-02T22:49:26.488739Z",
     "start_time": "2021-08-02T22:49:26.473482Z"
    }
   },
   "outputs": [],
   "source": [
    "mt_predictions = [\n",
    "    [\"the cat is on the mat\", \"There is cat playing on the mat\"], \n",
    "    [\"Look! a wonderful day.\"]\n",
    "]\n",
    "mt_references = [\n",
    "    [\"the cat is playing on the mat.\", \"The cat plays on the mat.\"],\n",
    "    [\"Today is a wonderful day\", \"The weather outside is wonderful.\"],\n",
    "]\n",
    "\n",
    "# mt_predictions = read_from_txt(\"/path/to/predictions.txt\")\n",
    "# mt_references = read_from_txt(\"/path/to/references.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Metrics\n",
    "\n",
    "Here define your metrics used to evaluate MT prediction and references. You can either use load function from jury where you can pass additional parameters to specified metric, or specify as string, which will use default parameters.\n",
    "\n",
    "**NOTE:** Computation of BERTScore may take some time as it will download a model for computing embeddings. Thus, we here provide `albert-base-v1`, but you can uncomment the previous line where it uses default model `roberta-large`.\n",
    "\n",
    "[Here](https://huggingface.co/transformers/pretrained_models.html), you can observe model sizes, parameter counts, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "MT_METRICS = [\n",
    "    load_metric(metric_name=\"bleu\", resulting_name=\"bleu_1\", params={\"max_order\": 1}),\n",
    "    load_metric(metric_name=\"bleu\", resulting_name=\"bleu_2\", params={\"max_order\": 2}),\n",
    "    load_metric(\"meteor\", \"meteor\"),\n",
    "    load_metric(\"rouge\"),\n",
    "    load_metric(\"sacrebleu\"),\n",
    "#     load_metric(\"bertscore\"), # Using default model for lang en\n",
    "    load_metric(\"bertscore\", params={\"model_type\": \"albert-base-v1\"})  # Using smaller model to reduce download time.\n",
    "]\n",
    "\n",
    "# Alternatively\n",
    "# MT_METRICS = [\n",
    "#     \"bleu\",\n",
    "#     \"meteor\",\n",
    "#     \"rouge\"\n",
    "# ]\n",
    "\n",
    "RUN_CONCURRENT = True  # set False to disable concurrency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-02T22:51:23.394106Z",
     "start_time": "2021-08-02T22:50:52.018613Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████| 2/2 [00:00<00:00, 49932.19it/s]\n",
      "  0%|                                                     | 0/2 [00:00<?, ?it/s][nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /home/devrimcavusoglu/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "100%|█████████████████████████████████████████████| 2/2 [00:03<00:00,  1.89s/it]\n",
      "100%|█████████████████████████████████████████████| 2/2 [00:04<00:00,  2.33s/it]\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /home/devrimcavusoglu/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "100%|█████████████████████████████████████████████| 2/2 [00:05<00:00,  2.65s/it]\n",
      "100%|█████████████████████████████████████████████| 2/2 [00:05<00:00,  2.67s/it]\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /home/devrimcavusoglu/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "100%|█████████████████████████████████████████████| 2/2 [00:05<00:00,  2.95s/it]\n",
      "100%|█████████████████████████████████████████████| 2/2 [00:31<00:00, 15.63s/it]\n"
     ]
    }
   ],
   "source": [
    "# Compute scores\n",
    "\n",
    "if RUN_CONCURRENT:\n",
    "    os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\"\n",
    "else:\n",
    "    os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "mt_jury = Jury(metrics=MT_METRICS, run_concurrent=RUN_CONCURRENT)\n",
    "scores = mt_jury.evaluate(predictions=mt_predictions, references=mt_references)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"empty_predictions\": 0,\n",
      "    \"total_items\": 2,\n",
      "    \"bleu_1\": 0.7920502936517768,\n",
      "    \"bleu_2\": 0.7225612529515497,\n",
      "    \"meteor\": 0.5420511682934044,\n",
      "    \"rougeL\": 0.7948717948717947,\n",
      "    \"SacreBLEU\": 0.3898310279399514,\n",
      "    \"BERTScore\": 0.7431023120880127\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Display results\n",
    "print(json.dumps(scores, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Question Answering\n",
    "\n",
    "For question answering task, commonly used evaluation metric is exact match or F1 score, datasets package allows this through a metric named \"squad\". Same interface is available here as well, with a single exception that in order to seamlessly compute, concat and output resulting scores Jury restrict each metric to compute a single score, by default SQUAD implementation computes (squad's) F1 score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_predictions = [\"1917\", \"Albert Einstein\", \"foo bar\"]\n",
    "qa_references = [\"1917\", \"Einstein\", \"foo bar foobar\"]\n",
    "\n",
    "QA_METRICS = [\n",
    "    \"squad\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████| 3/3 [00:00<00:00, 62291.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"empty_predictions\": 0,\n",
      "    \"total_items\": 3,\n",
      "    \"SQUAD\": 0.8222222222222223\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "qa_jury = Jury(metrics=QA_METRICS, run_concurrent=False)\n",
    "scores = qa_jury.evaluate(predictions=qa_predictions, references=qa_references)\n",
    "print(json.dumps(scores, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining a custom metric\n",
    "\n",
    "To define a custom metric, you only need to extend `jury.metrics.Metric` class and implement the required functions as desired. We create a metric to compute precision for our QA task above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from typing import Dict\n",
    "\n",
    "from jury.metrics import Metric\n",
    "\n",
    "\n",
    "class Precision(Metric):\n",
    "    \"\"\"\n",
    "    Compute simple precision as \n",
    "        Average( # of matching tokens / # of tokens in prediction )\n",
    "    \"\"\"\n",
    "    def __init__(self, metric_name: str = None, resulting_name: str = None, params: Dict = None):\n",
    "        metric_name = self.__class__.__name__ if metric_name is None else metric_name\n",
    "        resulting_name = metric_name if resulting_name is None else resulting_name\n",
    "        super().__init__(metric_name=metric_name, resulting_name=resulting_name, params=params)\n",
    "        \n",
    "    def _preprocess(self, predictions, references):\n",
    "        predictions = [p.split() for p in predictions]\n",
    "        references = [r.split() for r in references]\n",
    "        return predictions, references\n",
    "    \n",
    "    def _compute(self, predictions, references):\n",
    "        scores = []\n",
    "        for pred, ref in zip(predictions, references):\n",
    "            score = 0\n",
    "            pred_counts = Counter(pred)\n",
    "            ref_counts = Counter(ref)\n",
    "            for token, pred_count in pred_counts.items():\n",
    "                if token in ref_counts:\n",
    "                    score += min(pred_count, ref_counts[token])  # Intersection count\n",
    "            scores.append(score /  len(pred))\n",
    "        avg_score = sum(scores) / len(scores)\n",
    "        return {self.resulting_name: avg_score}\n",
    "    \n",
    "    def compute(self, predictions, references) -> Dict[str, float]:\n",
    "        \"\"\"Required to be used by jury.\"\"\"\n",
    "        predictions, references = self._preprocess(predictions, references)\n",
    "        return self._compute(predictions, references)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jury.metrics.squad import SQUAD\n",
    "\n",
    "QA_METRICS = [\n",
    "    SQUAD(),\n",
    "    Precision()\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████| 3/3 [00:00<00:00, 56679.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"empty_predictions\": 0,\n",
      "    \"total_items\": 3,\n",
      "    \"SQUAD\": 0.8222222222222223,\n",
      "    \"Precision\": 0.8333333333333334\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "qa_jury = Jury(metrics=QA_METRICS, run_concurrent=False)\n",
    "scores = qa_jury.evaluate(predictions=qa_predictions, references=qa_references)\n",
    "print(json.dumps(scores, indent=4))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
