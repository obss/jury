{
  "evaluate": {
    "empty_items": 0,
    "total_items": 2,
    "accuracy": {
      "score": 0.6571428571428571
    },
    "bertscore": {
      "score": 0.7350330352783203,
      "precision": [
        0.8328237533569336,
        0.6074944734573364
      ],
      "recall": [
        0.8258965015411377,
        0.6777911186218262
      ],
      "f1": [
        0.8293456435203552,
        0.6407204270362854
      ],
      "hashcode": "albert-base-v1_L10_no-idf_version=0.3.10(hug_trans=4.9.1)"
    },
    "bleu": {
      "score": 0.0,
      "precisions": [
        0.18181818181818182,
        0.0,
        0.0,
        0.0
      ],
      "brevity_penalty": 1.0,
      "length_ratio": 3.6666666666666665,
      "translation_length": 11,
      "reference_length": 3
    },
    "f1": {
      "score": 0.6926184355490269
    },
    "meteor": {
      "score": 0.6190249433106576
    },
    "precision": {
      "score": 0.7321428571428572
    },
    "recall": {
      "score": 0.6571428571428571
    },
    "rouge": {
      "rouge1": 0.6904761904761905,
      "rouge2": 0.45238095238095244,
      "rougeL": 0.619047619047619,
      "rougeLsum": 0.619047619047619
    },
    "sacrebleu": {
      "score": 0.42728700639623396,
      "counts": [
        7,
        4,
        2,
        1
      ],
      "totals": [
        8,
        7,
        6,
        5
      ],
      "precisions": [
        0.875,
        0.5714285714285715,
        0.33333333333333337,
        0.2
      ],
      "bp": 1.0,
      "sys_len": 8,
      "ref_len": 8
    },
    "squad": {
      "exact_match": 0.0,
      "f1": 0.6857142857142858
    }
  },
  "evaluate_concurrent": {
    "empty_items": 0,
    "total_items": 2,
    "accuracy": {
      "score": 0.6571428571428571
    },
    "bertscore": {
      "score": 0.7350330352783203,
      "precision": [
        0.8328237533569336,
        0.6074944734573364
      ],
      "recall": [
        0.8258965015411377,
        0.6777911186218262
      ],
      "f1": [
        0.8293456435203552,
        0.6407204270362854
      ],
      "hashcode": "albert-base-v1_L10_no-idf_version=0.3.10(hug_trans=4.9.1)"
    },
    "bleu": {
      "score": 0.0,
      "precisions": [
        0.18181818181818182,
        0.0,
        0.0,
        0.0
      ],
      "brevity_penalty": 1.0,
      "length_ratio": 3.6666666666666665,
      "translation_length": 11,
      "reference_length": 3
    },
    "f1": {
      "score": 0.6926184355490269
    },
    "meteor": {
      "score": 0.6190249433106576
    },
    "precision": {
      "score": 0.7321428571428572
    },
    "recall": {
      "score": 0.6571428571428571
    },
    "rouge": {
      "rouge1": 0.6904761904761905,
      "rouge2": 0.45238095238095244,
      "rougeL": 0.619047619047619,
      "rougeLsum": 0.619047619047619
    },
    "sacrebleu": {
      "score": 0.42728700639623396,
      "counts": [
        7,
        4,
        2,
        1
      ],
      "totals": [
        8,
        7,
        6,
        5
      ],
      "precisions": [
        0.875,
        0.5714285714285715,
        0.33333333333333337,
        0.2
      ],
      "bp": 1.0,
      "sys_len": 8,
      "ref_len": 8
    },
    "squad": {
      "exact_match": 0.0,
      "f1": 0.6857142857142858
    }
  },
  "evaluate_str_input": {
    "empty_items": 0,
    "total_items": 2,
    "bleu": {
      "score": 0.0,
      "precisions": [
        0.18181818181818182,
        0.0,
        0.0,
        0.0
      ],
      "brevity_penalty": 1.0,
      "length_ratio": 3.6666666666666665,
      "translation_length": 11,
      "reference_length": 3
    }
  },
  "evaluate_list_str_input": {
    "empty_items": 0,
    "total_items": 2,
    "accuracy": {
      "score": 0.6571428571428571
    },
    "bertscore": {
      "score": 0.9396075010299683,
      "precision": [
        0.9543163180351257,
        0.9277863502502441
      ],
      "recall": [
        0.9502041935920715,
        0.9261335134506226
      ],
      "f1": [
        0.9522558450698853,
        0.9269591569900513
      ],
      "hashcode": "roberta-large_L17_no-idf_version=0.3.10(hug_trans=4.9.1)"
    },
    "bleu": {
      "score": 0.0,
      "precisions": [
        0.18181818181818182,
        0.0,
        0.0,
        0.0
      ],
      "brevity_penalty": 1.0,
      "length_ratio": 3.6666666666666665,
      "translation_length": 11,
      "reference_length": 3
    },
    "f1": {
      "score": 0.6926184355490269
    },
    "meteor": {
      "score": 0.6190249433106576
    },
    "precision": {
      "score": 0.7321428571428572
    },
    "recall": {
      "score": 0.6571428571428571
    },
    "rouge": {
      "rouge1": 0.6904761904761905,
      "rouge2": 0.45238095238095244,
      "rougeL": 0.619047619047619,
      "rougeLsum": 0.619047619047619
    },
    "sacrebleu": {
      "score": 0.42728700639623396,
      "counts": [
        7,
        4,
        2,
        1
      ],
      "totals": [
        8,
        7,
        6,
        5
      ],
      "precisions": [
        0.875,
        0.5714285714285715,
        0.33333333333333337,
        0.2
      ],
      "bp": 1.0,
      "sys_len": 8,
      "ref_len": 8
    },
    "squad": {
      "exact_match": 0.0,
      "f1": 0.6857142857142858
    }
  },
  "evaluate_list_dict_input": {
    "empty_items": 0,
    "total_items": 2,
    "accuracy": {
      "score": 0.6571428571428571
    },
    "bertscore": {
      "score": 0.7350329756736755,
      "precision": [
        0.832823634147644,
        0.6074944734573364
      ],
      "recall": [
        0.8258962631225586,
        0.6777912378311157
      ],
      "f1": [
        0.8293454647064209,
        0.6407204866409302
      ],
      "hashcode": "albert-base-v1_L10_no-idf_version=0.3.10(hug_trans=4.9.1)"
    },
    "bleu-1": {
      "score": 0.18181818181818182,
      "precisions": [
        0.18181818181818182
      ],
      "brevity_penalty": 1.0,
      "length_ratio": 3.6666666666666665,
      "translation_length": 11,
      "reference_length": 3
    },
    "bleu-2": {
      "score": 0.0,
      "precisions": [
        0.18181818181818182,
        0.0
      ],
      "brevity_penalty": 1.0,
      "length_ratio": 3.6666666666666665,
      "translation_length": 11,
      "reference_length": 3
    },
    "F1": {
      "score": 0.6926184355490269
    },
    "METEOR": {
      "score": 0.6190249433106576
    },
    "precision": {
      "score": 0.7321428571428572
    },
    "recall": {
      "score": 0.6571428571428571
    },
    "rouge": {
      "rouge1": 0.6904761904761905,
      "rouge2": 0.45238095238095244,
      "rougeL": 0.619047619047619,
      "rougeLsum": 0.619047619047619
    },
    "sacrebleu": {
      "score": 0.42728700639623396,
      "counts": [
        7,
        4,
        2,
        1
      ],
      "totals": [
        8,
        7,
        6,
        5
      ],
      "precisions": [
        0.875,
        0.5714285714285715,
        0.33333333333333337,
        0.2
      ],
      "bp": 1.0,
      "sys_len": 8,
      "ref_len": 8
    },
    "squad": {
      "exact_match": 0.0,
      "f1": 0.6857142857142858
    }
  },
  "evaluate_list_mixed_input": {
    "empty_items": 0,
    "total_items": 2,
    "accuracy": {
      "score": 0.6571428571428571
    },
    "bertscore": {
      "score": 0.9396075010299683,
      "precision": [
        0.9543163180351257,
        0.9277863502502441
      ],
      "recall": [
        0.9502041935920715,
        0.9261335134506226
      ],
      "f1": [
        0.9522558450698853,
        0.9269591569900513
      ],
      "hashcode": "roberta-large_L17_no-idf_version=0.3.10(hug_trans=4.9.1)"
    },
    "bleu": {
      "score": 0.0,
      "precisions": [
        0.18181818181818182,
        0.0,
        0.0,
        0.0
      ],
      "brevity_penalty": 1.0,
      "length_ratio": 3.6666666666666665,
      "translation_length": 11,
      "reference_length": 3
    },
    "f1": {
      "score": 0.6926184355490269
    },
    "meteor": {
      "score": 0.6190249433106576
    },
    "precision": {
      "score": 0.7321428571428572
    },
    "recall": {
      "score": 0.6571428571428571
    },
    "rouge": {
      "rouge1": 0.6904761904761905,
      "rouge2": 0.45238095238095244,
      "rougeL": 0.619047619047619,
      "rougeLsum": 0.619047619047619
    },
    "sacrebleu": {
      "score": 0.42728700639623396,
      "counts": [
        7,
        4,
        2,
        1
      ],
      "totals": [
        8,
        7,
        6,
        5
      ],
      "precisions": [
        0.875,
        0.5714285714285715,
        0.33333333333333337,
        0.2
      ],
      "bp": 1.0,
      "sys_len": 8,
      "ref_len": 8
    },
    "squad": {
      "exact_match": 0.0,
      "f1": 0.6857142857142858
    }
  },
  "evaluate_datasets_metric": {
    "empty_items": 0,
    "total_items": 2,
    "wer": 0.5833333333333334
  },
  "evaluate_corpus": {
    "empty_items": 0,
    "total_items": 2,
    "accuracy": {
      "score": 0.7285714285714285
    },
    "bertscore": {
      "score": 0.7485644221305847,
      "precision": [
        0.8912335634231567,
        0.6074944734573364
      ],
      "recall": [
        0.8377017378807068,
        0.6777911186218262
      ],
      "f1": [
        0.856408417224884,
        0.6407204270362854
      ],
      "hashcode": "albert-base-v1_L10_no-idf_version=0.3.10(hug_trans=4.11.3)"
    },
    "bleu": {
      "score": 0.0,
      "precisions": [
        0.9,
        0.75,
        0.5,
        0.0
      ],
      "brevity_penalty": 0.9048374180359595,
      "length_ratio": 0.9090909090909091,
      "translation_length": 10,
      "reference_length": 11
    },
    "f1": {
      "score": 0.7948717948717947
    },
    "meteor": {
      "score": 0.727184593644221
    },
    "precision": {
      "score": 0.875
    },
    "recall": {
      "score": 0.7285714285714285
    },
    "rouge": {
      "rouge1": 0.7948717948717947,
      "rouge2": 0.6493506493506493,
      "rougeL": 0.7948717948717947,
      "rougeLsum": 0.7948717948717947
    },
    "sacrebleu": {
      "score": 0.2730120862709067,
      "counts": [
        8,
        4,
        2,
        0
      ],
      "totals": [
        12,
        10,
        8,
        6
      ],
      "precisions": [
        0.6666666666666667,
        0.4,
        0.25,
        0.08333333333333334
      ],
      "bp": 1.0,
      "sys_len": 12,
      "ref_len": 11
    },
    "squad": {
      "exact_match": 0.0,
      "f1": 0.7301587301587302
    }
  },
  "evaluate_multiple_predictions_empty": {
    "total_items": 2,
    "empty_items": 1,
    "accuracy": {
      "score": 0.8333333333333334
    },
    "bertscore": {
      "score": 0.9471233487129211,
      "precision": 0.9065421223640442,
      "recall": 0.9915080070495605,
      "f1": 0.9471233487129211,
      "hashcode": "albert-base-v1_L10_no-idf_version=0.3.10(hug_trans=4.9.1)"
    },
    "bleu": {
      "score": 0.0,
      "precisions": [
        0.75,
        0.5,
        0.25,
        0.0
      ],
      "brevity_penalty": 1.0,
      "length_ratio": 1.2,
      "translation_length": 6,
      "reference_length": 5
    },
    "f1": {
      "score": 0.9090909090909091
    },
    "meteor": {
      "score": 0.9490196078431373
    },
    "precision": {
      "score": 0.8333333333333334
    },
    "recall": {
      "score": 1.0
    },
    "rouge": {
      "rouge1": 0.8181818181818181,
      "rouge2": 0.5555555555555555,
      "rougeL": 0.7272727272727273,
      "rougeLsum": 0.7272727272727273
    },
    "sacrebleu": {
      "score": 0.2537202001116631,
      "counts": [
        5,
        2,
        1,
        0
      ],
      "totals": [
        15,
        13,
        11,
        9
      ],
      "precisions": [
        0.33333333333333337,
        0.15384615384615385,
        0.09090909090909091,
        0.05555555555555555
      ],
      "bp": 1.0,
      "sys_len": 6,
      "ref_len": 11,
      "adjusted_precisions": [
        0.6666666666666667,
        0.3076923076923077,
        0.18181818181818182,
        0.1111111111111111
      ]
    },
    "squad": {
      "exact_match": 0.0,
      "f1": 0.888888888888889
    }
  },
  "evaluate_multiple_predictions": {
    "empty_items": 0,
    "total_items": 2,
    "accuracy": {
      "score": 0.8452380952380952
    },
    "bertscore": {
      "score": 0.9017658531665802,
      "precision": 0.8988877832889557,
      "recall": 0.9146048426628113,
      "f1": 0.9017658531665802,
      "hashcode": "albert-base-v1_L10_no-idf_version=0.3.10(hug_trans=4.9.1)"
    },
    "bleu": {
      "score": 0.0,
      "precisions": [
        0.8333333333333334,
        0.5,
        0.25,
        0.0
      ],
      "brevity_penalty": 1.0,
      "length_ratio": 1.0909090909090908,
      "translation_length": 12,
      "reference_length": 11
    },
    "f1": {
      "score": 0.916083916083916
    },
    "meteor": {
      "score": 0.9012408828265606
    },
    "precision": {
      "score": 0.9166666666666667
    },
    "recall": {
      "score": 0.9285714285714286
    },
    "rouge": {
      "rouge1": 0.8321678321678321,
      "rouge2": 0.505050505050505,
      "rougeL": 0.7482517482517482,
      "rougeLsum": 0.7482517482517482
    },
    "sacrebleu": {
      "score": 0.5773502691896256,
      "counts": [
        10,
        4,
        2,
        0
      ],
      "totals": [
        12,
        10,
        8,
        6
      ],
      "precisions": [
        0.8333333333333333,
        0.4,
        0.25,
        0.08333333333333334
      ],
      "bp": 1.0,
      "sys_len": 12,
      "ref_len": 11,
      "adjusted_precisions": [
        1.6666666666666665,
        0.8,
        0.5,
        0.16666666666666669
      ]
    },
    "squad": {
      "exact_match": 0.0,
      "f1": 0.898989898989899
    }
  },
  "evaluate_sequence_classification": {
    "total_items": 6,
    "empty_items": 0,
    "accuracy": {
      "score": 0.3333333333333333
    },
    "f1": {
      "score": [
        0.8,
        0.0,
        0.0
      ]
    },
    "precision": {
      "score": [
        0.6666666666666666,
        0.0,
        0.0
      ]
    },
    "recall": {
      "recall": [
        1.0,
        0.0,
        0.0
      ]
    }
  }
}